{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b81b7a67",
   "metadata": {},
   "source": [
    "## extrat_chunks_json ë³€í™˜ ì €ì¥"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14a4385c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pdfplumber\n",
    "import re\n",
    "import json\n",
    "from typing import List, Dict\n",
    "\n",
    "def extract_text_from_pdf(pdf_path: str) -> str:\n",
    "    \"\"\"PDF íŒŒì¼ì—ì„œ ì „ì²´ í…ìŠ¤íŠ¸ ì¶”ì¶œ\"\"\"\n",
    "    text = \"\"\n",
    "    with pdfplumber.open(pdf_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            page_text = page.extract_text()\n",
    "            if page_text:\n",
    "                text += \"\\n\" + page_text\n",
    "    return text\n",
    "\n",
    "def chunk_grammar_rules(text: str) -> List[Dict]:\n",
    "    \"\"\"ì–´ë¬¸ ê·œë²” ë¬¸ì„œë¥¼ ê·œì¹™ ë‹¨ìœ„ë¡œ ì²­í‚¹\"\"\"\n",
    "    chunks = []\n",
    "    pattern = r\"<([^>]+)>\"\n",
    "    split_parts = re.split(pattern, text)\n",
    "    for i in range(1, len(split_parts), 2):\n",
    "        title = split_parts[i].strip()\n",
    "        content = split_parts[i + 1].strip()\n",
    "\n",
    "        category = title.split(\" - \")[0].strip()\n",
    "        rule_number_match = re.search(r\"ì œ\\d+í•­\", title)\n",
    "        rule_number = rule_number_match.group() if rule_number_match else None\n",
    "\n",
    "        examples = re.findall(r\"^- (.+)\", content, flags=re.MULTILINE)\n",
    "        notes =  re.findall(r\"((?:\\[ë¶™ì„ \\d+\\]|ë‹¤ë§Œ)[\\s\\S]+?)(?=\\[ë¶™ì„|\\Z)\", content)\n",
    "        pairs = re.findall(r\"ã„±: (.+?)\\n\\s*ã„´: (.+)\", content, flags=re.DOTALL)\n",
    "\n",
    "        first_line = content.split(\"\\n\")[0]\n",
    "        main_rule = first_line.strip()\n",
    "\n",
    "        chunks.append({\n",
    "            \"title\": title,\n",
    "            \"category\": category,\n",
    "            \"rule_number\": rule_number,\n",
    "            \"main_rule\": main_rule,\n",
    "            \"examples\": examples,\n",
    "            \"notes\": [note.strip() for note in notes],\n",
    "            \"pairs\": [{\"correct\": g.strip(), \"wrong\": n.strip()} for g, n in pairs],\n",
    "            \"source\": \"êµ­ì–´ ì§€ì‹ ê¸°ë°˜ ìƒì„±(RAG) ì°¸ì¡° ë¬¸ì„œ\"\n",
    "        })\n",
    "    return chunks\n",
    "\n",
    "def main():\n",
    "    pdf_path = \"/home/jiin/korean_grammar_rag/data/document.pdf\"\n",
    "    text = extract_text_from_pdf(pdf_path)\n",
    "    chunks = chunk_grammar_rules(text)\n",
    "\n",
    "    # 4. JSONìœ¼ë¡œ ì €ì¥\n",
    "    output_path = \"korean_grammar_chunks.json\"\n",
    "    with open(output_path, \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(chunks, f, ensure_ascii=False, indent=2)\n",
    "\n",
    "    print(f\"[âœ”] ì´ {len(chunks)}ê°œì˜ ê·œì¹™ì´ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9249a104",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ì´ ì²­í¬ ìˆ˜: 115\n",
      "\n",
      "ì²« ë²ˆì§¸ ì²­í¬ êµ¬ì¡°:\n",
      "{\n",
      "  \"title\": \"ë„ì–´ì“°ê¸° - í•œê¸€ ë§ì¶¤ë²• ì œ43í•­\",\n",
      "  \"category\": \"ë„ì–´ì“°ê¸°\",\n",
      "  \"rule_number\": \"ì œ43í•­\",\n",
      "  \"main_rule\": \"ë‹¨ìœ„ë¥¼ ë‚˜íƒ€ë‚´ëŠ” ëª…ì‚¬ëŠ” ë„ì–´ ì“´ë‹¤.\",\n",
      "  \"examples\": [\n",
      "    \"í•œ ê°œ, ì°¨ í•œ ëŒ€, ê¸ˆ ì„œ ëˆ, ì†Œ í•œ ë§ˆë¦¬, ì˜· í•œ ë²Œ, ì—´ ì‚´, ì¡°ê¸° í•œ ì†, ì—°í•„ í•œ ìë£¨, ë²„\",\n",
      "    \"ë‘ì‹œ ì‚¼ì‹­ë¶„ ì˜¤ì´ˆ, ì œì¼ê³¼, ì‚¼í•™ë…„, ìœ¡ì¸µ, 1446ë…„ 10ì›” 9ì¼, 2ëŒ€ëŒ€, 16ë™ 502í˜¸, ì œ1ì‹¤ìŠµ\"\n",
      "  ],\n",
      "  \"notes\": [\n",
      "    \"ë‹¤ë§Œ, ìˆœì„œë¥¼ ë‚˜íƒ€ë‚´ëŠ” ê²½ìš°ë‚˜ ìˆ«ìì™€ ì–´ìš¸ë ¤ ì“°ì´ëŠ” ê²½ìš°ì—ëŠ” ë¶™ì—¬ ì“¸ ìˆ˜ ìˆë‹¤.\\n- ë‘ì‹œ ì‚¼ì‹­ë¶„ ì˜¤ì´ˆ, ì œì¼ê³¼, ì‚¼í•™ë…„, ìœ¡ì¸µ, 1446ë…„ 10ì›” 9ì¼, 2ëŒ€ëŒ€, 16ë™ 502í˜¸, ì œ1ì‹¤ìŠµ\\nì‹¤, 80ì›, 10ê°œ, 7ë¯¸í„°\"\n",
      "  ],\n",
      "  \"pairs\": [],\n",
      "  \"source\": \"êµ­ì–´ ì§€ì‹ ê¸°ë°˜ ìƒì„±(RAG) ì°¸ì¡° ë¬¸ì„œ\"\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "# ì²­í‚¹ ê²°ê³¼ í™•ì¸\n",
    "\n",
    "with open(\"/home/jiin/korean_grammar_rag/code/korean_grammar_chunks.json\", \"r\", encoding=\"utf-8\") as f:\n",
    "    chunks = json.load(f)\n",
    "\n",
    "print(f\"ì´ ì²­í¬ ìˆ˜: {len(chunks)}\")\n",
    "print(\"\\nì²« ë²ˆì§¸ ì²­í¬ êµ¬ì¡°:\")\n",
    "print(json.dumps(chunks[3], ensure_ascii=False, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f97a9b",
   "metadata": {},
   "source": [
    "## ì„ë² ë”© í›„ ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d018323",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'# ì‚¬ìš© ì˜ˆì‹œ\\ndef main():\\n    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\\n    rag = SimpleKoreanRAG()\\n    \\n    # ì²­í¬ ë¡œë“œ\\n    rag.load_chunks(\"korean_grammar_chunks.json\")\\n    \\n    # ì„ë² ë”© ìƒì„±\\n    rag.create_embeddings()\\n    \\n    # ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•\\n    rag.build_vector_store()\\n    \\n    # ì €ì¥\\n    rag.save(\"./rag_system\")\\n    \\n    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰\\n    print(\"\\n=== ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===\")\\n    test_queries = [\\n        \"ë¨¹ì´ì–‘ ë¨¹ì´ëŸ‰\",\\n        \"ë°”ë˜ìš” ë°”ë¼ìš”\", \\n        \"ë„ì–´ì“°ê¸°\",\\n        \"ë‘ìŒë²•ì¹™\"\\n    ]\\n    \\n    for query in test_queries:\\n        print(f\"\\nğŸ” ì¿¼ë¦¬: \\'{query}\\'\")\\n        results = rag.search(query, top_k=3)\\n        \\n        for i, (chunk, score) in enumerate(results):\\n            print(f\"  {i+1}. [{score:.3f}] {chunk[\\'title\\']}\")\\n            print(f\"     ê·œì¹™: {chunk[\\'main_rule\\']}\")\\n\\nif __name__ == \"__main__\":\\n    main()'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import faiss\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "from typing import List, Dict, Tuple\n",
    "\n",
    "class SimpleKoreanRAG:\n",
    "    def __init__(self):\n",
    "        \"\"\"ê°„ë‹¨í•œ í•œêµ­ì–´ RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\"\"\"\n",
    "        print(\"í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\")\n",
    "        \n",
    "        # í•œêµ­ì–´ íŠ¹í™” ì„ë² ë”© ëª¨ë¸ ë¡œë“œ\n",
    "        self.model = SentenceTransformer('jhgan/ko-sroberta-multitask')\n",
    "        \n",
    "        # GPU ì‚¬ìš© ê°€ëŠ¥í•˜ë©´ GPUë¡œ\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        self.model.to(device)\n",
    "        print(f\"ì‚¬ìš© ë””ë°”ì´ìŠ¤: {device}\")\n",
    "        \n",
    "        self.chunks = []\n",
    "        self.embeddings = None\n",
    "        self.index = None\n",
    "    \n",
    "    def load_chunks(self, json_path: str):\n",
    "        \"\"\"ì²­í¬ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        with open(json_path, 'r', encoding='utf-8') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        print(f\"ì²­í¬ ë¡œë“œ ì™„ë£Œ: {len(self.chunks)}ê°œ\")\n",
    "    \n",
    "    def create_text_for_embedding(self, chunk: Dict) -> str:\n",
    "        \"\"\"ê° ì²­í¬ë¥¼ ì„ë² ë”©ìš© í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\"\"\"\n",
    "        # ê¸°ë³¸ ì •ë³´\n",
    "        title = chunk.get('title', '')\n",
    "        category = chunk.get('category', '')\n",
    "        main_rule = chunk.get('main_rule', '')\n",
    "        \n",
    "        # ì˜ˆì‹œë“¤ í•©ì¹˜ê¸°\n",
    "        examples = chunk.get('examples', [])\n",
    "        examples_text = ' '.join(examples) if examples else ''\n",
    "        \n",
    "        # ì£¼ì˜ì‚¬í•­ë“¤ í•©ì¹˜ê¸°  \n",
    "        notes = chunk.get('notes', [])\n",
    "        notes_text = ' '.join(notes) if notes else ''\n",
    "        \n",
    "        # ì˜¬ë°”ë¥¸/í‹€ë¦° ìŒë“¤ í•©ì¹˜ê¸°\n",
    "        pairs = chunk.get('pairs', [])\n",
    "        pairs_text = ''\n",
    "        for pair in pairs:\n",
    "            correct = pair.get('correct', '')\n",
    "            wrong = pair.get('wrong', '')\n",
    "            pairs_text += f' {correct} {wrong}'\n",
    "        \n",
    "        # ëª¨ë“  í…ìŠ¤íŠ¸ í•©ì¹˜ê¸°\n",
    "        full_text = f\"{title} {category} {main_rule} {examples_text} {notes_text} {pairs_text}\"\n",
    "        return full_text.strip()\n",
    "    \n",
    "    def create_embeddings(self):\n",
    "        \"\"\"ëª¨ë“  ì²­í¬ì˜ ì„ë² ë”© ìƒì„±\"\"\"\n",
    "        print(\"ì„ë² ë”© ìƒì„± ì¤‘...\")\n",
    "        \n",
    "        # ê° ì²­í¬ë¥¼ í…ìŠ¤íŠ¸ë¡œ ë³€í™˜\n",
    "        texts = []\n",
    "        for i, chunk in enumerate(self.chunks):\n",
    "            text = self.create_text_for_embedding(chunk)\n",
    "            texts.append(text)\n",
    "            \n",
    "            # ì²˜ìŒ 3ê°œë§Œ í™•ì¸ìš© ì¶œë ¥\n",
    "            if i < 3:\n",
    "                print(f\"\\nì²­í¬ {i+1} í…ìŠ¤íŠ¸ ìƒ˜í”Œ:\")\n",
    "                print(text[:200] + \"...\")\n",
    "        \n",
    "        # ì„ë² ë”© ìƒì„±\n",
    "        self.embeddings = self.model.encode(texts, show_progress_bar=True)\n",
    "        print(f\"ì„ë² ë”© ìƒì„± ì™„ë£Œ: {self.embeddings.shape}\")\n",
    "        \n",
    "        return self.embeddings\n",
    "    \n",
    "    def build_vector_store(self):\n",
    "        \"\"\"FAISS ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•\"\"\"\n",
    "        if self.embeddings is None:\n",
    "            raise ValueError(\"ë¨¼ì € ì„ë² ë”©ì„ ìƒì„±í•´ì£¼ì„¸ìš”\")\n",
    "        \n",
    "        print(\"ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì¤‘...\")\n",
    "        \n",
    "        # ì„ë² ë”© ì°¨ì›\n",
    "        dim = self.embeddings.shape[1]\n",
    "        print(f\"ë²¡í„° ì°¨ì›: {dim}\")\n",
    "        \n",
    "        # FAISS ì¸ë±ìŠ¤ ìƒì„± (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ìš©)\n",
    "        self.index = faiss.IndexFlatIP(dim)\n",
    "        \n",
    "        # ì„ë² ë”© ì •ê·œí™” (ì½”ì‚¬ì¸ ìœ ì‚¬ë„ë¥¼ ìœ„í•´)\n",
    "        embeddings_normalized = self.embeddings.copy().astype('float32')\n",
    "        faiss.normalize_L2(embeddings_normalized)\n",
    "        \n",
    "        # ë²¡í„° ì¶”ê°€\n",
    "        self.index.add(embeddings_normalized)\n",
    "        \n",
    "        print(f\"ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶• ì™„ë£Œ: {self.index.ntotal}ê°œ ë²¡í„°\")\n",
    "    \n",
    "    def search(self, query: str, top_k: int = 5) -> List[Tuple[Dict, float]]:\n",
    "        \"\"\"ì¿¼ë¦¬ë¡œ ìœ ì‚¬í•œ ì²­í¬ ê²€ìƒ‰\"\"\"\n",
    "        if self.index is None:\n",
    "            raise ValueError(\"ë¨¼ì € ë²¡í„° ì €ì¥ì†Œë¥¼ êµ¬ì¶•í•´ì£¼ì„¸ìš”\")\n",
    "        \n",
    "        # ì¿¼ë¦¬ ì„ë² ë”© ìƒì„±\n",
    "        query_embedding = self.model.encode([query]).astype('float32')\n",
    "        faiss.normalize_L2(query_embedding)\n",
    "        \n",
    "        # ê²€ìƒ‰ ìˆ˜í–‰\n",
    "        scores, indices = self.index.search(query_embedding, top_k)\n",
    "        \n",
    "        # ê²°ê³¼ ë°˜í™˜\n",
    "        results = []\n",
    "        for score, idx in zip(scores[0], indices[0]):\n",
    "            if idx < len(self.chunks):\n",
    "                chunk = self.chunks[idx]\n",
    "                results.append((chunk, float(score)))\n",
    "        \n",
    "        return results\n",
    "    \n",
    "    def save(self, save_dir: str = \"./rag_system\"):\n",
    "        \"\"\"ì‹œìŠ¤í…œ ì €ì¥\"\"\"\n",
    "        import os\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "        \n",
    "        # ì²­í¬ ì €ì¥\n",
    "        with open(f\"{save_dir}/chunks.json\", 'w', encoding='utf-8') as f:\n",
    "            json.dump(self.chunks, f, ensure_ascii=False, indent=2)\n",
    "        \n",
    "        # ì„ë² ë”© ì €ì¥\n",
    "        np.save(f\"{save_dir}/embeddings.npy\", self.embeddings)\n",
    "        \n",
    "        # FAISS ì¸ë±ìŠ¤ ì €ì¥\n",
    "        faiss.write_index(self.index, f\"{save_dir}/index.faiss\")\n",
    "        \n",
    "        print(f\"ì‹œìŠ¤í…œì´ {save_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
    "    \n",
    "    def load(self, save_dir: str = \"./rag_system\"):\n",
    "        \"\"\"ì €ì¥ëœ ì‹œìŠ¤í…œ ë¡œë“œ\"\"\"\n",
    "        # ì²­í¬ ë¡œë“œ\n",
    "        with open(f\"{save_dir}/chunks.json\", 'r', encoding='utf-8') as f:\n",
    "            self.chunks = json.load(f)\n",
    "        \n",
    "        # ì„ë² ë”© ë¡œë“œ\n",
    "        self.embeddings = np.load(f\"{save_dir}/embeddings.npy\")\n",
    "        \n",
    "        # FAISS ì¸ë±ìŠ¤ ë¡œë“œ\n",
    "        self.index = faiss.read_index(f\"{save_dir}/index.faiss\")\n",
    "        \n",
    "        print(f\"ì‹œìŠ¤í…œì´ {save_dir}ì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤\")\n",
    "\n",
    "'''# ì‚¬ìš© ì˜ˆì‹œ\n",
    "def main():\n",
    "    # RAG ì‹œìŠ¤í…œ ì´ˆê¸°í™”\n",
    "    rag = SimpleKoreanRAG()\n",
    "    \n",
    "    # ì²­í¬ ë¡œë“œ\n",
    "    rag.load_chunks(\"korean_grammar_chunks.json\")\n",
    "    \n",
    "    # ì„ë² ë”© ìƒì„±\n",
    "    rag.create_embeddings()\n",
    "    \n",
    "    # ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•\n",
    "    rag.build_vector_store()\n",
    "    \n",
    "    # ì €ì¥\n",
    "    rag.save(\"./rag_system\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ê²€ìƒ‰\n",
    "    print(\"\\n=== ê²€ìƒ‰ í…ŒìŠ¤íŠ¸ ===\")\n",
    "    test_queries = [\n",
    "        \"ë¨¹ì´ì–‘ ë¨¹ì´ëŸ‰\",\n",
    "        \"ë°”ë˜ìš” ë°”ë¼ìš”\", \n",
    "        \"ë„ì–´ì“°ê¸°\",\n",
    "        \"ë‘ìŒë²•ì¹™\"\n",
    "    ]\n",
    "    \n",
    "    for query in test_queries:\n",
    "        print(f\"\\nğŸ” ì¿¼ë¦¬: '{query}'\")\n",
    "        results = rag.search(query, top_k=3)\n",
    "        \n",
    "        for i, (chunk, score) in enumerate(results):\n",
    "            print(f\"  {i+1}. [{score:.3f}] {chunk['title']}\")\n",
    "            print(f\"     ê·œì¹™: {chunk['main_rule']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aea91119",
   "metadata": {},
   "source": [
    "### RAG ì‹œìŠ¤í…œ êµ¬ì¶•"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "db1f23e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiin/miniconda3/envs/k_rag/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def main():\\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\\n    \\n    # ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ë¡œë“œ (ì´ë¯¸ êµ¬ì¶•ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\\n    from __main__ import rag  # ì´ë¯¸ ë§Œë“  rag ì‹œìŠ¤í…œ ì‚¬ìš©\\n    \\n    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\\n    pipeline = KoreanGrammarRAGPipeline(\\n        rag_system=rag,\\n        model_name=\"Qwen/Qwen2.5-3B-Instruct\"  # ë˜ëŠ” ë‹¤ë¥¸ í•œêµ­ì–´ ëª¨ë¸\\n    )\\n    \\n    # í›ˆë ¨ ë°ì´í„° ë¡œë“œ\\n    pipeline.load_training_data(\"/home/jiin/korean_grammar_rag/data/korean_language_rag_V1.0_train.json\")\\n    \\n    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰\\n    pipeline.test_on_training_data(num_samples=3)\\n    \\n    # ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\\n    print(\"\\n=== ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ===\")\\n    test_question = \"\"ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤.\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\"\\n    \\n    result = pipeline.process_question(test_question)\\n    print(f\"ì§ˆë¬¸: {result[\\'question\\']}\")\\n    print(f\"ë‹µë³€: {result[\\'final_answer\\']}\")\\n\\nif __name__ == \"__main__\":\\n    main()'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "from typing import List, Dict, Tuple\n",
    "import re\n",
    "\n",
    "class KoreanGrammarRAGPipeline:\n",
    "    def __init__(self, rag_system, model_name: str = \"Qwen/Qwen2.5-3B-Instruct\"):\n",
    "        \"\"\"\n",
    "        RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
    "        Args:\n",
    "            rag_system: ì´ë¯¸ êµ¬ì¶•ëœ SimpleKoreanRAG ì‹œìŠ¤í…œ\n",
    "            model_name: ìƒì„±ì— ì‚¬ìš©í•  ì–¸ì–´ëª¨ë¸\n",
    "        \"\"\"\n",
    "        self.rag_system = rag_system\n",
    "        \n",
    "        print(f\"ìƒì„± ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}\")\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì €ì™€ ëª¨ë¸ ë¡œë“œ\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float16 if torch.cuda.is_available() else torch.float32,\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None\n",
    "        )\n",
    "        \n",
    "        # íŒ¨ë”© í† í° ì„¤ì •\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        print(\"RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ\")\n",
    "    \n",
    "    def load_training_data(self, train_json_path: str):\n",
    "        \"\"\"í›ˆë ¨ ë°ì´í„° ë¡œë“œ ë° í™•ì¸\"\"\"\n",
    "        with open(train_json_path, 'r', encoding='utf-8') as f:\n",
    "            self.train_data = json.load(f)\n",
    "        \n",
    "        print(f\"í›ˆë ¨ ë°ì´í„° ë¡œë“œ ì™„ë£Œ: {len(self.train_data)}ê°œ\")\n",
    "        \n",
    "        # ì²« ë²ˆì§¸ ìƒ˜í”Œ êµ¬ì¡° í™•ì¸\n",
    "        if self.train_data:\n",
    "            print(\"\\nì²« ë²ˆì§¸ í›ˆë ¨ ìƒ˜í”Œ:\")\n",
    "            print(json.dumps(self.train_data[0], ensure_ascii=False, indent=2))\n",
    "        \n",
    "        return self.train_data\n",
    "    \n",
    "    def retrieve_relevant_chunks(self, question: str, top_k: int = 3) -> List[Dict]:\n",
    "        \"\"\"ì§ˆë¬¸ì— ê´€ë ¨ëœ ì²­í¬ ê²€ìƒ‰\"\"\"\n",
    "        # RAG ì‹œìŠ¤í…œìœ¼ë¡œ ê´€ë ¨ ì²­í¬ ê²€ìƒ‰\n",
    "        results = self.rag_system.search(question, top_k=top_k)\n",
    "        \n",
    "        # ì²­í¬ë§Œ ì¶”ì¶œ (ì ìˆ˜ ì œê±°)\n",
    "        relevant_chunks = [chunk for chunk, score in results]\n",
    "        \n",
    "        return relevant_chunks\n",
    "    \n",
    "    def format_retrieved_context(self, chunks: List[Dict]) -> str:\n",
    "        \"\"\"ê²€ìƒ‰ëœ ì²­í¬ë“¤ì„ ì»¨í…ìŠ¤íŠ¸ ë¬¸ìì—´ë¡œ í¬ë§·íŒ…\"\"\"\n",
    "        context_parts = []\n",
    "        \n",
    "        for i, chunk in enumerate(chunks, 1):\n",
    "            context = f\"[ì°¸ê³  ê·œì¹™ {i}]\\n\"\n",
    "            context += f\"ì œëª©: {chunk['title']}\\n\"\n",
    "            context += f\"ê·œì¹™: {chunk['main_rule']}\\n\"\n",
    "            \n",
    "            # ì˜ˆì‹œê°€ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if chunk.get('examples'):\n",
    "                context += f\"ì˜ˆì‹œ: {', '.join(chunk['examples'][:2])}\\n\"  # ì²˜ìŒ 2ê°œë§Œ\n",
    "            \n",
    "            # ì˜¬ë°”ë¥¸/í‹€ë¦° ìŒì´ ìˆìœ¼ë©´ ì¶”ê°€\n",
    "            if chunk.get('pairs'):\n",
    "                for pair in chunk['pairs'][:2]:  # ì²˜ìŒ 2ê°œë§Œ\n",
    "                    context += f\"ì˜¬ë°”ë¥¸ í‘œí˜„: {pair.get('correct', '')}\\n\"\n",
    "                    context += f\"í‹€ë¦° í‘œí˜„: {pair.get('wrong', '')}\\n\"\n",
    "            \n",
    "            context_parts.append(context)\n",
    "        \n",
    "        return \"\\n\".join(context_parts)\n",
    "    \n",
    "    def create_prompt(self, question: str, context: str) -> str:\n",
    "        \"\"\"ì§ˆë¬¸ê³¼ ì»¨í…ìŠ¤íŠ¸ë¡œ í”„ë¡¬í”„íŠ¸ ìƒì„±\"\"\"\n",
    "        \n",
    "        # ì§ˆë¬¸ ìœ í˜• íŒŒì•…\n",
    "        question_type = \"ì„ íƒí˜•\" if \"{\" in question and \"}\" in question else \"êµì •í˜•\"\n",
    "        \n",
    "        prompt = f\"\"\"ë‹¹ì‹ ì€ í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì „ë¬¸ê°€ì…ë‹ˆë‹¤. ì£¼ì–´ì§„ ì°¸ê³  ê·œì¹™ì„ ë°”íƒ•ìœ¼ë¡œ ì§ˆë¬¸ì— ì •í™•íˆ ë‹µë³€í•´ì£¼ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ê·œì¹™:\n",
    "{context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€ í˜•ì‹: \"{{ì •ë‹µ}}ì´/ê°€ ì˜³ë‹¤. {{ì´ìœ }}\"\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "        \n",
    "        return prompt\n",
    "    \n",
    "    def generate_answer(self, prompt: str, max_length: int = 300) -> str:\n",
    "        \"\"\"í”„ë¡¬í”„íŠ¸ë¡œë¶€í„° ë‹µë³€ ìƒì„±\"\"\"\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì§•\n",
    "        inputs = self.tokenizer(\n",
    "            prompt,\n",
    "            return_tensors=\"pt\",\n",
    "            truncation=True,\n",
    "            max_length=1024,\n",
    "            padding=True\n",
    "        )\n",
    "        \n",
    "        # GPUë¡œ ì´ë™\n",
    "        if torch.cuda.is_available():\n",
    "            inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "        \n",
    "        # ìƒì„±\n",
    "        with torch.no_grad():\n",
    "            outputs = self.model.generate(\n",
    "                **inputs,\n",
    "                max_new_tokens=max_length,\n",
    "                temperature=0.7,\n",
    "                do_sample=True,\n",
    "                pad_token_id=self.tokenizer.eos_token_id,\n",
    "                eos_token_id=self.tokenizer.eos_token_id\n",
    "            )\n",
    "        \n",
    "        # ë””ì½”ë”©\n",
    "        generated_text = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "        \n",
    "        # í”„ë¡¬í”„íŠ¸ ë¶€ë¶„ ì œê±°í•˜ê³  ë‹µë³€ë§Œ ì¶”ì¶œ\n",
    "        answer = generated_text[len(prompt):].strip()\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def post_process_answer(self, answer: str) -> str:\n",
    "        \"\"\"ë‹µë³€ í›„ì²˜ë¦¬ (í˜•ì‹ ë§ì¶”ê¸°)\"\"\"\n",
    "        \n",
    "        # ë¶ˆí•„ìš”í•œ ë¶€ë¶„ ì œê±°\n",
    "        answer = answer.strip()\n",
    "        \n",
    "        # ì¤„ë°”ê¿ˆì„ ê³µë°±ìœ¼ë¡œ ë³€ê²½\n",
    "        answer = re.sub(r'\\n+', ' ', answer)\n",
    "        \n",
    "        # ì¤‘ë³µ ê³µë°± ì œê±°\n",
    "        answer = re.sub(r'\\s+', ' ', answer)\n",
    "        \n",
    "        # í˜•ì‹ í™•ì¸ ë° ìˆ˜ì •\n",
    "        if not ((\"ì´ ì˜³ë‹¤\" in answer) or (\"ê°€ ì˜³ë‹¤\" in answer)):\n",
    "            # í˜•ì‹ì´ ë§ì§€ ì•Šìœ¼ë©´ ê¸°ë³¸ í˜•ì‹ìœ¼ë¡œ ê°ìŒˆ\n",
    "            if answer:\n",
    "                answer = f\"{answer}ì´ ì˜³ë‹¤.\"\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    def process_question(self, question: str, retrieve_top_k: int = 3) -> Dict:\n",
    "        \"\"\"ì „ì²´ RAG í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        # 1. ê´€ë ¨ ì²­í¬ ê²€ìƒ‰\n",
    "        relevant_chunks = self.retrieve_relevant_chunks(question, top_k=retrieve_top_k)\n",
    "        \n",
    "        # 2. ì»¨í…ìŠ¤íŠ¸ í¬ë§·íŒ…\n",
    "        context = self.format_retrieved_context(relevant_chunks)\n",
    "        \n",
    "        # 3. í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        prompt = self.create_prompt(question, context)\n",
    "        \n",
    "        # 4. ë‹µë³€ ìƒì„±\n",
    "        raw_answer = self.generate_answer(prompt)\n",
    "        \n",
    "        # 5. í›„ì²˜ë¦¬\n",
    "        final_answer = self.post_process_answer(raw_answer)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"retrieved_chunks\": relevant_chunks,\n",
    "            \"context\": context,\n",
    "            \"prompt\": prompt,\n",
    "            \"raw_answer\": raw_answer,\n",
    "            \"final_answer\": final_answer\n",
    "        }\n",
    "    \n",
    "    def evaluate_on_sample(self, sample: Dict) -> Dict:\n",
    "        \"\"\"ë‹¨ì¼ ìƒ˜í”Œì— ëŒ€í•œ í‰ê°€\"\"\"\n",
    "        \n",
    "        question = sample[\"input\"][\"question\"]\n",
    "        ground_truth = sample[\"output\"][\"answer\"]\n",
    "        \n",
    "        # RAG í”„ë¡œì„¸ìŠ¤ ì‹¤í–‰\n",
    "        result = self.process_question(question)\n",
    "        \n",
    "        return {\n",
    "            \"question\": question,\n",
    "            \"ground_truth\": ground_truth,\n",
    "            \"predicted\": result[\"final_answer\"],\n",
    "            \"retrieved_chunks\": result[\"retrieved_chunks\"],\n",
    "            \"context\": result[\"context\"]\n",
    "        }\n",
    "    \n",
    "    def test_on_training_data(self, num_samples: int = 5):\n",
    "        \"\"\"í›ˆë ¨ ë°ì´í„°ì˜ ì¼ë¶€ë¡œ í…ŒìŠ¤íŠ¸\"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'train_data'):\n",
    "            print(\"ë¨¼ì € í›ˆë ¨ ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš”.\")\n",
    "            return\n",
    "        \n",
    "        print(f\"\\n=== {num_samples}ê°œ ìƒ˜í”Œ í…ŒìŠ¤íŠ¸ ===\")\n",
    "        \n",
    "        for i in range(min(num_samples, len(self.train_data))):\n",
    "            sample = self.train_data[i]\n",
    "            result = self.evaluate_on_sample(sample)\n",
    "            \n",
    "            print(f\"\\n--- ìƒ˜í”Œ {i+1} ---\")\n",
    "            print(f\"ì§ˆë¬¸: {result['question']}\")\n",
    "            print(f\"ì •ë‹µ: {result['ground_truth']}\")\n",
    "            print(f\"ì˜ˆì¸¡: {result['predicted']}\")\n",
    "            print(f\"ê²€ìƒ‰ëœ ì²­í¬ ìˆ˜: {len(result['retrieved_chunks'])}\")\n",
    "            \n",
    "            # ê²€ìƒ‰ëœ ì²« ë²ˆì§¸ ì²­í¬ ì œëª©ë§Œ ì¶œë ¥\n",
    "            if result['retrieved_chunks']:\n",
    "                print(f\"ê°€ì¥ ê´€ë ¨ì„± ë†’ì€ ê·œì¹™: {result['retrieved_chunks'][0]['title']}\")\n",
    "\n",
    "'''def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # ê¸°ì¡´ RAG ì‹œìŠ¤í…œ ë¡œë“œ (ì´ë¯¸ êµ¬ì¶•ë˜ì–´ ìˆë‹¤ê³  ê°€ì •)\n",
    "    from __main__ import rag  # ì´ë¯¸ ë§Œë“  rag ì‹œìŠ¤í…œ ì‚¬ìš©\n",
    "    \n",
    "    # RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
    "    pipeline = KoreanGrammarRAGPipeline(\n",
    "        rag_system=rag,\n",
    "        model_name=\"Qwen/Qwen2.5-3B-Instruct\"  # ë˜ëŠ” ë‹¤ë¥¸ í•œêµ­ì–´ ëª¨ë¸\n",
    "    )\n",
    "    \n",
    "    # í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n",
    "    pipeline.load_training_data(\"/home/jiin/korean_grammar_rag/data/korean_language_rag_V1.0_train.json\")\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸ ì‹¤í–‰\n",
    "    pipeline.test_on_training_data(num_samples=3)\n",
    "    \n",
    "    # ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸\n",
    "    print(\"\\n=== ê°œë³„ ì§ˆë¬¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "    test_question = \"\\\"ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤.\\\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "    \n",
    "    result = pipeline.process_question(test_question)\n",
    "    print(f\"ì§ˆë¬¸: {result['question']}\")\n",
    "    print(f\"ë‹µë³€: {result['final_answer']}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "940f6e0c",
   "metadata": {},
   "source": [
    "### Train í›ˆë ¨"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "9fb8589a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/jiin/miniconda3/envs/k_rag/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'def main():\\n    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\\n    \\n    # RAG ì‹œìŠ¤í…œ ë¡œë“œ (ì„ íƒì‚¬í•­ - ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ìš©)\\n    try:\\n        from __main__ import rag  # ê¸°ì¡´ì— ë§Œë“  rag ì‹œìŠ¤í…œ\\n        print(\"RAG ì‹œìŠ¤í…œì„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\\n        rag_system = rag\\n    except:\\n        print(\"RAG ì‹œìŠ¤í…œ ì—†ì´ í›ˆë ¨í•©ë‹ˆë‹¤.\")\\n        rag_system = None\\n    \\n    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\\n    trainer = KoreanGrammarTrainer(\\n        model_name=\"Qwen/Qwen2.5-3B-Instruct\",  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸\\n        rag_system=rag_system,\\n        output_dir=\"./korean_grammar_model\"\\n    )\\n    \\n    # ë°ì´í„° ë¡œë“œ\\n    train_dataset, val_dataset = trainer.load_data(\\n        train_path=\"train.json\",\\n        val_path=\"validation.json\"  # ì—†ìœ¼ë©´ ìë™ ë¶„í• \\n    )\\n    \\n    # í›ˆë ¨ ì‹¤í–‰\\n    trainer.train(\\n        num_epochs=3,\\n        batch_size=2,  # RTX 4090ì— ë§ê²Œ ì¡°ì •\\n        learning_rate=5e-5\\n    )\\n    \\n    # í…ŒìŠ¤íŠ¸\\n    test_questions = [\\n        \"\"ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤.\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\",\\n        \"ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ë ‡ê²Œ ê³ ì¹œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”. \"ì–´ì„œ ì¾Œì°¨í•˜ì‹œê¸¸ ë°”ë˜ìš”.\"\"\\n    ]\\n    \\n    trainer.test_generation(test_questions)\\n\\nif __name__ == \"__main__\":\\n    main()'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import json\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from transformers import (\n",
    "    AutoTokenizer, \n",
    "    AutoModelForCausalLM,\n",
    "    TrainingArguments,\n",
    "    Trainer,\n",
    "    DataCollatorForLanguageModeling\n",
    ")\n",
    "from typing import List, Dict\n",
    "import os\n",
    "\n",
    "class KoreanGrammarDataset(Dataset):\n",
    "    \"\"\"í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ë°ì´í„°ì…‹\"\"\"\n",
    "    \n",
    "    def __init__(self, data: List[Dict], tokenizer, rag_system=None, max_length: int = 512):\n",
    "        self.data = data\n",
    "        self.tokenizer = tokenizer\n",
    "        self.rag_system = rag_system\n",
    "        self.max_length = max_length\n",
    "        \n",
    "        # í›ˆë ¨ìš© í”„ë¡¬í”„íŠ¸ ìƒì„±\n",
    "        self.processed_data = self._process_data()\n",
    "    \n",
    "    def _get_relevant_context(self, question: str) -> str:\n",
    "        \"\"\"ì§ˆë¬¸ì— ê´€ë ¨ëœ ì»¨í…ìŠ¤íŠ¸ ê²€ìƒ‰ (RAG ì‹œìŠ¤í…œ ì‚¬ìš©)\"\"\"\n",
    "        if self.rag_system is None:\n",
    "            return \"\"\n",
    "        \n",
    "        try:\n",
    "            # ê´€ë ¨ ì²­í¬ ê²€ìƒ‰ (ìƒìœ„ 2ê°œë§Œ)\n",
    "            results = self.rag_system.search(question, top_k=2)\n",
    "            \n",
    "            context_parts = []\n",
    "            for chunk, score in results:\n",
    "                context = f\"ì°¸ê³ : {chunk['title']} - {chunk['main_rule']}\"\n",
    "                if chunk.get('examples'):\n",
    "                    context += f\" ì˜ˆì‹œ: {chunk['examples'][0]}\"\n",
    "                context_parts.append(context)\n",
    "            \n",
    "            return \" \".join(context_parts)\n",
    "        except:\n",
    "            return \"\"\n",
    "    \n",
    "    def _process_data(self):\n",
    "        \"\"\"ë°ì´í„° ì „ì²˜ë¦¬\"\"\"\n",
    "        processed = []\n",
    "        \n",
    "        for item in self.data:\n",
    "            question = item[\"input\"][\"question\"]\n",
    "            answer = item[\"output\"][\"answer\"]\n",
    "            \n",
    "            # RAG ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ (ì„ íƒì‚¬í•­)\n",
    "            context = self._get_relevant_context(question)\n",
    "            \n",
    "            # í”„ë¡¬í”„íŠ¸ êµ¬ì„±\n",
    "            if context:\n",
    "                prompt = f\"\"\"ë‹¤ìŒ ê·œì¹™ì„ ì°¸ê³ í•˜ì—¬ ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì°¸ê³  ê·œì¹™: {context}\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€: {answer}\"\"\"\n",
    "            else:\n",
    "                prompt = f\"\"\"í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€: {answer}\"\"\"\n",
    "            \n",
    "            processed.append({\n",
    "                \"text\": prompt,\n",
    "                \"question\": question,\n",
    "                \"answer\": answer\n",
    "            })\n",
    "        \n",
    "        return processed\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.processed_data)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        item = self.processed_data[idx]\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì§•\n",
    "        encoding = self.tokenizer(\n",
    "            item[\"text\"],\n",
    "            truncation=True,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            return_tensors=\"pt\"\n",
    "        )\n",
    "        \n",
    "        return {\n",
    "            \"input_ids\": encoding[\"input_ids\"].flatten(),\n",
    "            \"attention_mask\": encoding[\"attention_mask\"].flatten(),\n",
    "            \"labels\": encoding[\"input_ids\"].flatten()  # ì–¸ì–´ ëª¨ë¸ë§ì—ì„œëŠ” input_idsê°€ labels\n",
    "        }\n",
    "\n",
    "class KoreanGrammarTrainer:\n",
    "    \"\"\"í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ëª¨ë¸ í›ˆë ¨ í´ë˜ìŠ¤\"\"\"\n",
    "    \n",
    "    def __init__(self, \n",
    "                 model_name: str = \"Qwen/Qwen2.5-3B-Instruct\",\n",
    "                 rag_system=None,\n",
    "                 output_dir: str = \"./korean_grammar_model\"):\n",
    "        \n",
    "        self.model_name = model_name\n",
    "        self.rag_system = rag_system\n",
    "        self.output_dir = output_dir\n",
    "        \n",
    "        print(f\"ëª¨ë¸ ë¡œë“œ ì¤‘: {model_name}\")\n",
    "        \n",
    "        # í† í¬ë‚˜ì´ì € ë¡œë“œ\n",
    "        self.tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "        if self.tokenizer.pad_token is None:\n",
    "            self.tokenizer.pad_token = self.tokenizer.eos_token\n",
    "        \n",
    "        # ëª¨ë¸ ë¡œë“œ\n",
    "        self.model = AutoModelForCausalLM.from_pretrained(\n",
    "            model_name,\n",
    "            torch_dtype=torch.float32,  # FP32 ì‚¬ìš©ìœ¼ë¡œ ì•ˆì •ì„± í™•ë³´\n",
    "            device_map=\"auto\" if torch.cuda.is_available() else None,\n",
    "            trust_remote_code=True\n",
    "        )\n",
    "        \n",
    "        print(\"ëª¨ë¸ ë¡œë“œ ì™„ë£Œ\")\n",
    "    \n",
    "    def load_data(self, train_path: str, val_path: str = None):\n",
    "        \"\"\"í›ˆë ¨ ë° ê²€ì¦ ë°ì´í„° ë¡œë“œ\"\"\"\n",
    "        print(f\"í›ˆë ¨ ë°ì´í„° ë¡œë“œ: {train_path}\")\n",
    "        \n",
    "        with open(train_path, 'r', encoding='utf-8') as f:\n",
    "            train_data = json.load(f)\n",
    "        \n",
    "        print(f\"í›ˆë ¨ ë°ì´í„°: {len(train_data)}ê°œ\")\n",
    "        \n",
    "        # ê²€ì¦ ë°ì´í„° (ìˆìœ¼ë©´ ë¡œë“œ, ì—†ìœ¼ë©´ í›ˆë ¨ ë°ì´í„°ì˜ 20% ì‚¬ìš©)\n",
    "        if val_path and os.path.exists(val_path):\n",
    "            with open(val_path, 'r', encoding='utf-8') as f:\n",
    "                val_data = json.load(f)\n",
    "            print(f\"ê²€ì¦ ë°ì´í„°: {len(val_data)}ê°œ\")\n",
    "        else:\n",
    "            # í›ˆë ¨ ë°ì´í„°ì˜ 20%ë¥¼ ê²€ì¦ìš©ìœ¼ë¡œ ë¶„í• \n",
    "            split_idx = int(len(train_data) * 0.8)\n",
    "            val_data = train_data[split_idx:]\n",
    "            train_data = train_data[:split_idx]\n",
    "            print(f\"ë°ì´í„° ë¶„í•  - í›ˆë ¨: {len(train_data)}ê°œ, ê²€ì¦: {len(val_data)}ê°œ\")\n",
    "        \n",
    "        # ë°ì´í„°ì…‹ ìƒì„±\n",
    "        self.train_dataset = KoreanGrammarDataset(\n",
    "            train_data, \n",
    "            self.tokenizer, \n",
    "            self.rag_system\n",
    "        )\n",
    "        \n",
    "        self.val_dataset = KoreanGrammarDataset(\n",
    "            val_data, \n",
    "            self.tokenizer, \n",
    "            self.rag_system\n",
    "        )\n",
    "        \n",
    "        return self.train_dataset, self.val_dataset\n",
    "    \n",
    "    def setup_training_args(self, \n",
    "                           num_epochs: int = 3,\n",
    "                           batch_size: int = 4,\n",
    "                           learning_rate: float = 5e-5,\n",
    "                           warmup_steps: int = 100):\n",
    "        \"\"\"í›ˆë ¨ ì„¤ì •\"\"\"\n",
    "        \n",
    "        training_args = TrainingArguments(\n",
    "            output_dir=self.output_dir,\n",
    "            overwrite_output_dir=True,\n",
    "            \n",
    "            # ê¸°ë³¸ ì„¤ì •\n",
    "            num_train_epochs=num_epochs,\n",
    "            per_device_train_batch_size=batch_size,\n",
    "            per_device_eval_batch_size=batch_size,\n",
    "            \n",
    "            # í•™ìŠµë¥  ë° ì˜µí‹°ë§ˆì´ì €\n",
    "            learning_rate=learning_rate,\n",
    "            warmup_steps=warmup_steps,\n",
    "            weight_decay=0.01,\n",
    "            \n",
    "            # ë¡œê¹… ë° ì €ì¥\n",
    "            logging_dir=f\"{self.output_dir}/logs\",\n",
    "            logging_steps=50,\n",
    "            save_steps=500,\n",
    "            save_total_limit=1,\n",
    "            \n",
    "            # í‰ê°€\n",
    "            evaluation_strategy=\"steps\",\n",
    "            eval_steps=100,\n",
    "            \n",
    "            # GPU ìµœì í™” - FP16 ë¹„í™œì„±í™”\n",
    "            fp16=False,  # FP16 ë¬¸ì œ í•´ê²°ì„ ìœ„í•´ ë¹„í™œì„±í™”\n",
    "            bf16=False,  # BF16ë„ ë¹„í™œì„±í™”\n",
    "            dataloader_pin_memory=False,\n",
    "            \n",
    "            # ê¸°íƒ€\n",
    "            remove_unused_columns=False,\n",
    "            report_to=None,  # wandb ë“± ì‚¬ìš© ì•ˆí•¨\n",
    "        )\n",
    "        \n",
    "        return training_args\n",
    "    \n",
    "    def train(self, \n",
    "              num_epochs: int = 3,\n",
    "              batch_size: int = 4,\n",
    "              learning_rate: float = 5e-5):\n",
    "        \"\"\"ëª¨ë¸ í›ˆë ¨ ì‹¤í–‰\"\"\"\n",
    "        \n",
    "        if not hasattr(self, 'train_dataset'):\n",
    "            raise ValueError(\"ë¨¼ì € ë°ì´í„°ë¥¼ ë¡œë“œí•´ì£¼ì„¸ìš” (load_data ë©”ì„œë“œ ì‚¬ìš©)\")\n",
    "        \n",
    "        print(\"í›ˆë ¨ ì‹œì‘...\")\n",
    "        \n",
    "        # í›ˆë ¨ ì„¤ì •\n",
    "        training_args = self.setup_training_args(\n",
    "            num_epochs=num_epochs,\n",
    "            batch_size=batch_size,\n",
    "            learning_rate=learning_rate\n",
    "        )\n",
    "        \n",
    "        # ë°ì´í„° ì½œë ˆì´í„°\n",
    "        data_collator = DataCollatorForLanguageModeling(\n",
    "            tokenizer=self.tokenizer,\n",
    "            mlm=False  # ìë™íšŒê·€ ì–¸ì–´ ëª¨ë¸ë§\n",
    "        )\n",
    "        \n",
    "        # íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "        trainer = Trainer(\n",
    "            model=self.model,\n",
    "            args=training_args,\n",
    "            train_dataset=self.train_dataset,\n",
    "            eval_dataset=self.val_dataset,\n",
    "            data_collator=data_collator,\n",
    "            tokenizer=self.tokenizer,\n",
    "        )\n",
    "        \n",
    "        # í›ˆë ¨ ì‹¤í–‰\n",
    "        trainer.train()\n",
    "        \n",
    "        # ìµœì¢… ëª¨ë¸ ì €ì¥\n",
    "        trainer.save_model(self.output_dir)\n",
    "        self.tokenizer.save_pretrained(self.output_dir)\n",
    "        \n",
    "        print(f\"í›ˆë ¨ ì™„ë£Œ! ëª¨ë¸ì´ {self.output_dir}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "        \n",
    "        return trainer\n",
    "    \n",
    "    def test_generation(self, test_questions: List[str]):\n",
    "        \"\"\"í›ˆë ¨ëœ ëª¨ë¸ë¡œ ìƒì„± í…ŒìŠ¤íŠ¸\"\"\"\n",
    "        print(\"\\n=== ìƒì„± í…ŒìŠ¤íŠ¸ ===\")\n",
    "        \n",
    "        for question in test_questions:\n",
    "            prompt = f\"\"\"í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "            \n",
    "            # í† í¬ë‚˜ì´ì§•\n",
    "            inputs = self.tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "            \n",
    "            if torch.cuda.is_available():\n",
    "                inputs = {k: v.to(self.model.device) for k, v in inputs.items()}\n",
    "            \n",
    "            # ìƒì„±\n",
    "            with torch.no_grad():\n",
    "                outputs = self.model.generate(\n",
    "                    **inputs,\n",
    "                    max_new_tokens=150,\n",
    "                    temperature=0.7,\n",
    "                    do_sample=True,\n",
    "                    pad_token_id=self.tokenizer.eos_token_id\n",
    "                )\n",
    "            \n",
    "            # ë””ì½”ë”©\n",
    "            generated = self.tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "            answer = generated[len(prompt):].strip()\n",
    "            \n",
    "            print(f\"\\nì§ˆë¬¸: {question}\")\n",
    "            print(f\"ë‹µë³€: {answer}\")\n",
    "\n",
    "'''def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # RAG ì‹œìŠ¤í…œ ë¡œë“œ (ì„ íƒì‚¬í•­ - ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ìš©)\n",
    "    try:\n",
    "        from __main__ import rag  # ê¸°ì¡´ì— ë§Œë“  rag ì‹œìŠ¤í…œ\n",
    "        print(\"RAG ì‹œìŠ¤í…œì„ ì»¨í…ìŠ¤íŠ¸ ì¶”ê°€ìš©ìœ¼ë¡œ ì‚¬ìš©í•©ë‹ˆë‹¤.\")\n",
    "        rag_system = rag\n",
    "    except:\n",
    "        print(\"RAG ì‹œìŠ¤í…œ ì—†ì´ í›ˆë ¨í•©ë‹ˆë‹¤.\")\n",
    "        rag_system = None\n",
    "    \n",
    "    # íŠ¸ë ˆì´ë„ˆ ì´ˆê¸°í™”\n",
    "    trainer = KoreanGrammarTrainer(\n",
    "        model_name=\"Qwen/Qwen2.5-3B-Instruct\",  # ë˜ëŠ” ë‹¤ë¥¸ ëª¨ë¸\n",
    "        rag_system=rag_system,\n",
    "        output_dir=\"./korean_grammar_model\"\n",
    "    )\n",
    "    \n",
    "    # ë°ì´í„° ë¡œë“œ\n",
    "    train_dataset, val_dataset = trainer.load_data(\n",
    "        train_path=\"train.json\",\n",
    "        val_path=\"validation.json\"  # ì—†ìœ¼ë©´ ìë™ ë¶„í• \n",
    "    )\n",
    "    \n",
    "    # í›ˆë ¨ ì‹¤í–‰\n",
    "    trainer.train(\n",
    "        num_epochs=3,\n",
    "        batch_size=2,  # RTX 4090ì— ë§ê²Œ ì¡°ì •\n",
    "        learning_rate=5e-5\n",
    "    )\n",
    "    \n",
    "    # í…ŒìŠ¤íŠ¸\n",
    "    test_questions = [\n",
    "        \"\\\"ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤.\\\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\",\n",
    "        \"ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ë ‡ê²Œ ê³ ì¹œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”. \\\"ì–´ì„œ ì¾Œì°¨í•˜ì‹œê¸¸ ë°”ë˜ìš”.\\\"\"\n",
    "    ]\n",
    "    \n",
    "    trainer.test_generation(test_questions)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()'''"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9486ca59",
   "metadata": {},
   "source": [
    "## ì‹¤í–‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "294a3bd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RAG ì‹œìŠ¤í…œ ë§Œë“¤ê¸°\n",
    "\n",
    "# 1. ë¨¼ì € SimpleKoreanRAG ì‹œìŠ¤í…œ ìƒì„±\n",
    "rag = SimpleKoreanRAG()\n",
    "\n",
    "# 2. ì²­í¬ ë¡œë“œ\n",
    "rag.load_chunks(\"/home/jiin/korean_grammar_rag/code/korean_grammar_chunks.json\")\n",
    "\n",
    "# 3. ì„ë² ë”© ìƒì„±\n",
    "rag.create_embeddings()\n",
    "\n",
    "# 4. ë²¡í„° ì €ì¥ì†Œ êµ¬ì¶•\n",
    "rag.build_vector_store()\n",
    "\n",
    "# 5. ì €ì¥ (ë‚˜ì¤‘ì— ì¬ì‚¬ìš©í•˜ê¸° ìœ„í•´)\n",
    "rag.save(\"./rag_system\")\n",
    "\n",
    "print(\"RAG ì‹œìŠ¤í…œ êµ¬ì¶• ì™„ë£Œ!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c59e36b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 6. ì´ì œ RAGë¥¼ í¬í•¨í•œ íŠ¸ë ˆì´ë„ˆ ìƒì„±\n",
    "trainer = KoreanGrammarTrainer(\n",
    "    model_name=\"Qwen/Qwen2.5-3B-Instruct\",\n",
    "    rag_system=rag,  # ë°©ê¸ˆ ë§Œë“  RAG ì‹œìŠ¤í…œ\n",
    "    output_dir=\"./korean_grammar_model\"\n",
    ")\n",
    "\n",
    "# 7. í›ˆë ¨ ë°ì´í„° ë¡œë“œ\n",
    "trainer.load_data(\"/home/jiin/korean_grammar_rag/data/korean_language_rag_V1.0_train.json\")\n",
    "\n",
    "# 8. í›ˆë ¨ ì‹¤í–‰\n",
    "trainer.train(num_epochs=3, batch_size=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "40644d97",
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_trained_model_fast(question):\n",
    "    \"\"\"ë¹ ë¥¸ ì¶”ë¡ ì„ ìœ„í•œ ì„¤ì •\"\"\"\n",
    "    prompt = f\"\"\"í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” ì§ˆë¬¸ì— ë‹µí•˜ì„¸ìš”.\n",
    "\n",
    "ì§ˆë¬¸: {question}\n",
    "\n",
    "ë‹µë³€:\"\"\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=256)  # ê¸¸ì´ ì¤„ì„\n",
    "    \n",
    "    # GPUë¡œ ì´ë™\n",
    "    if torch.cuda.is_available():\n",
    "        inputs = {k: v.cuda() for k, v in inputs.items()}\n",
    "        model.cuda()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        outputs = model.generate(\n",
    "            **inputs,\n",
    "            max_new_tokens=50,      # í† í° ìˆ˜ ëŒ€í­ ì¤„ì„\n",
    "            temperature=0.3,        # ë” ê²°ì •ì ìœ¼ë¡œ\n",
    "            do_sample=False,        # ìƒ˜í”Œë§ ë” (greedy)\n",
    "            num_beams=1,           # ë¹” ì„œì¹˜ ë„ê¸°\n",
    "            pad_token_id=tokenizer.eos_token_id,\n",
    "            eos_token_id=tokenizer.eos_token_id\n",
    "        )\n",
    "    \n",
    "    generated = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "    answer = generated[len(prompt):].strip()\n",
    "    \n",
    "    return answer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7235f847",
   "metadata": {},
   "outputs": [],
   "source": [
    "rag = SimpleKoreanRAG()\n",
    "\n",
    "# ì €ì¥ëœ ì‹œìŠ¤í…œ ë¡œë“œ\n",
    "rag.load(\"./rag_system\")\n",
    "\n",
    "print(\"RAG ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ!\")\n",
    "\n",
    "test_questions = [\n",
    "    \"\\\"ê°€ì¶•ì„ ê¸°ë¥¼ ë•Œì—ëŠ” {ë¨¹ì´ëŸ‰/ë¨¹ì´ì–‘}ì„ ì¡°ì ˆí•´ ì£¼ì–´ì•¼ í•œë‹¤.\\\" ê°€ìš´ë° ì˜¬ë°”ë¥¸ ê²ƒì„ ì„ íƒí•˜ê³ , ê·¸ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”.\",\n",
    "    \"ë‹¤ìŒ ë¬¸ì¥ì—ì„œ ì–´ë¬¸ ê·œë²”ì— ë¶€í•©í•˜ì§€ ì•ŠëŠ” ë¶€ë¶„ì„ ì°¾ì•„ ê³ ì¹˜ê³ , ê·¸ë ‡ê²Œ ê³ ì¹œ ì´ìœ ë¥¼ ì„¤ëª…í•˜ì„¸ìš”. \\\"ì–´ì„œ ì¾Œì°¨í•˜ì‹œê¸¸ ë°”ë˜ìš”.\\\"\",\n",
    "    \"ë„ì–´ì“°ê¸° ê·œì¹™ì— ëŒ€í•´ ì„¤ëª…í•˜ì„¸ìš”.\"\n",
    "]\n",
    "\n",
    "# ê¸°ì¡´ RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œë„ í…ŒìŠ¤íŠ¸ (ë¹„êµìš©)\n",
    "pipeline = KoreanGrammarRAGPipeline(rag_system=rag, model_name=\"./korean_grammar_model\")\n",
    "\n",
    "print(\"\\n=== RAG íŒŒì´í”„ë¼ì¸ í…ŒìŠ¤íŠ¸ ===\")\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    result = pipeline.process_question(question)\n",
    "    print(f\"\\n{i}. ì§ˆë¬¸: {question}\")\n",
    "    print(f\"   ë‹µë³€: {result['final_answer']}\")\n",
    "    print(f\"   ê²€ìƒ‰ëœ ê·œì¹™: {result['retrieved_chunks'][0]['title'] if result['retrieved_chunks'] else 'None'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04d5cff7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...\n",
      "í•œêµ­ì–´ ì„ë² ë”© ëª¨ë¸ ë¡œë“œ ì¤‘...\n",
      "ì‚¬ìš© ë””ë°”ì´ìŠ¤: cuda\n",
      "ì‹œìŠ¤í…œì´ ./rag_systemì—ì„œ ë¡œë“œë˜ì—ˆìŠµë‹ˆë‹¤\n",
      "RAG ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ!\n",
      "ìƒì„± ëª¨ë¸ ë¡œë“œ ì¤‘: ./korean_grammar_model\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards:  33%|â–ˆâ–ˆâ–ˆâ–      | 1/3 [00:01<00:03,  1.72s/it]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 3/3 [00:04<00:00,  1.57s/it]\n",
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RAG íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™” ì™„ë£Œ\n",
      "\n",
      "1. RAG + íŒŒì¸íŠœë‹ ëª¨ë¸ ì¶”ë¡ \n",
      "Validation ë°ì´í„° ë¡œë“œ: /home/jiin/korean_grammar_rag/data/korean_language_rag_V1.0_dev.json\n",
      "ì´ 127ê°œ ìƒ˜í”Œ\n",
      "ì¶”ë¡  ì‹œì‘...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ì¶”ë¡  ì¤‘:   0%|          | 0/127 [00:00<?, ?it/s]"
     ]
    }
   ],
   "source": [
    "import json\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "\n",
    "def run_inference_on_validation(rag_pipeline, val_data_path: str, output_path: str = \"validation_predictions.json\"):\n",
    "    \"\"\"\n",
    "    Validation ë°ì´í„°ì— RAG íŒŒì´í”„ë¼ì¸ì„ ì ìš©í•˜ì—¬ ì˜ˆì¸¡ ê²°ê³¼ ìƒì„±\n",
    "    \n",
    "    Args:\n",
    "        rag_pipeline: KoreanGrammarRAGPipeline ì¸ìŠ¤í„´ìŠ¤\n",
    "        val_data_path: validation ë°ì´í„° íŒŒì¼ ê²½ë¡œ\n",
    "        output_path: ê²°ê³¼ ì €ì¥í•  íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    \n",
    "    # Validation ë°ì´í„° ë¡œë“œ\n",
    "    print(f\"Validation ë°ì´í„° ë¡œë“œ: {val_data_path}\")\n",
    "    with open(val_data_path, 'r', encoding='utf-8') as f:\n",
    "        val_data = json.load(f)\n",
    "    \n",
    "    print(f\"ì´ {len(val_data)}ê°œ ìƒ˜í”Œ\")\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ ì €ì¥í•  ë¦¬ìŠ¤íŠ¸\n",
    "    predictions = []\n",
    "    \n",
    "    # ê° ìƒ˜í”Œì— ëŒ€í•´ ì¶”ë¡  ìˆ˜í–‰\n",
    "    print(\"ì¶”ë¡  ì‹œì‘...\")\n",
    "    for sample in tqdm(val_data, desc=\"ì¶”ë¡  ì¤‘\"):\n",
    "        sample_id = sample[\"id\"]\n",
    "        question = sample[\"input\"][\"question\"]\n",
    "        \n",
    "        try:\n",
    "            # RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ë‹µë³€ ìƒì„±\n",
    "            result = rag_pipeline.process_question(question)\n",
    "            predicted_answer = result['final_answer']\n",
    "            \n",
    "            # ê²°ê³¼ ì €ì¥ (idì™€ outputë§Œ)\n",
    "            predictions.append({\n",
    "                \"id\": sample_id,\n",
    "                \"output\": {\n",
    "                    \"answer\": predicted_answer\n",
    "                }\n",
    "            })\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"ìƒ˜í”Œ {sample_id} ì¶”ë¡  ì‹¤íŒ¨: {e}\")\n",
    "            # ì‹¤íŒ¨í•œ ê²½ìš° ë¹ˆ ë‹µë³€\n",
    "            predictions.append({\n",
    "                \"id\": sample_id,\n",
    "                \"output\": {\n",
    "                    \"answer\": \"ë‹µë³€ ìƒì„± ì‹¤íŒ¨\"\n",
    "                }\n",
    "            })\n",
    "    \n",
    "    # ê²°ê³¼ ì €ì¥\n",
    "    with open(output_path, 'w', encoding='utf-8') as f:\n",
    "        json.dump(predictions, f, ensure_ascii=False, indent=2)\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡ ê²°ê³¼ê°€ {output_path}ì— ì €ì¥ë˜ì—ˆìŠµë‹ˆë‹¤.\")\n",
    "    return predictions\n",
    "\n",
    "def evaluate_with_official_metric(predictions_path: str, ground_truth_path: str):\n",
    "    \"\"\"\n",
    "    ê³µì‹ í‰ê°€ ì½”ë“œë¥¼ ì‚¬ìš©í•˜ì—¬ ì„±ëŠ¥ í‰ê°€\n",
    "    \n",
    "    Args:\n",
    "        predictions_path: ì˜ˆì¸¡ ê²°ê³¼ íŒŒì¼ ê²½ë¡œ\n",
    "        ground_truth_path: ì •ë‹µ íŒŒì¼ ê²½ë¡œ\n",
    "    \"\"\"\n",
    "    \n",
    "    # ì˜ˆì¸¡ ê²°ê³¼ì™€ ì •ë‹µ ë¡œë“œ\n",
    "    with open(predictions_path, 'r', encoding='utf-8') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    with open(ground_truth_path, 'r', encoding='utf-8') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    print(f\"ì˜ˆì¸¡ ë°ì´í„°: {len(predictions)}ê°œ\")\n",
    "    print(f\"ì •ë‹µ ë°ì´í„°: {len(ground_truth)}ê°œ\")\n",
    "    \n",
    "    # ê³µì‹ í‰ê°€ í•¨ìˆ˜ ì‚¬ìš© (ì²¨ë¶€ëœ ì½”ë“œì—ì„œ)\n",
    "    try:\n",
    "        # ì—¬ê¸°ì„œ ì²¨ë¶€ëœ evaluation í•¨ìˆ˜ë¥¼ import í•´ì•¼ í•¨\n",
    "        # íŒŒì¼ì´ ê°™ì€ ë””ë ‰í† ë¦¬ì— ìˆë‹¤ê³  ê°€ì •\n",
    "        from evaluation_metrics import evaluation  # ì²¨ë¶€ëœ íŒŒì¼ì„ evaluation_metrics.pyë¡œ ì €ì¥í–ˆë‹¤ê³  ê°€ì •\n",
    "        \n",
    "        # í•œêµ­ì–´ ì–´ë¬¸ ê·œë²” RAG í‰ê°€ ì‹¤í–‰\n",
    "        results = evaluation(\n",
    "            inferenced_data=predictions,\n",
    "            ground_truth=ground_truth,\n",
    "            evaluation_metrics=['korean_contest_RAG_QA'],\n",
    "            ratio=1,\n",
    "            iteration=1\n",
    "        )\n",
    "        \n",
    "        return results\n",
    "        \n",
    "    except ImportError:\n",
    "        print(\"ê³µì‹ í‰ê°€ ì½”ë“œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ìˆ˜ë™ìœ¼ë¡œ í‰ê°€í•©ë‹ˆë‹¤.\")\n",
    "        return manual_evaluation(predictions, ground_truth)\n",
    "\n",
    "def manual_evaluation(predictions, ground_truth):\n",
    "    \"\"\"\n",
    "    ê³µì‹ í‰ê°€ ì½”ë“œê°€ ì—†ì„ ê²½ìš° ìˆ˜ë™ í‰ê°€\n",
    "    \"\"\"\n",
    "    # IDë¡œ ë§¤ì¹­\n",
    "    pred_dict = {item['id']: item['output']['answer'] for item in predictions}\n",
    "    true_dict = {item['id']: item['output']['answer'] for item in ground_truth}\n",
    "    \n",
    "    # Exact Match ê³„ì‚° (ê°„ë‹¨ ë²„ì „)\n",
    "    exact_matches = 0\n",
    "    total = 0\n",
    "    \n",
    "    for item_id in true_dict:\n",
    "        if item_id in pred_dict:\n",
    "            true_answer = true_dict[item_id].strip()\n",
    "            pred_answer = pred_dict[item_id].strip()\n",
    "            \n",
    "            # \"{ì •ë‹µ}ì´/ê°€ ì˜³ë‹¤\" ë¶€ë¶„ ì¶”ì¶œí•´ì„œ ë¹„êµ\n",
    "            true_main = extract_main_answer(true_answer)\n",
    "            pred_main = extract_main_answer(pred_answer)\n",
    "            \n",
    "            if true_main == pred_main:\n",
    "                exact_matches += 1\n",
    "            \n",
    "            total += 1\n",
    "    \n",
    "    exact_match_score = exact_matches / total if total > 0 else 0\n",
    "    \n",
    "    return {\n",
    "        \"exact_match\": exact_match_score,\n",
    "        \"exact_match_percent\": exact_match_score * 100,\n",
    "        \"total_samples\": total,\n",
    "        \"correct_samples\": exact_matches\n",
    "    }\n",
    "\n",
    "def extract_main_answer(answer_text):\n",
    "    \"\"\"ë‹µë³€ì—ì„œ ì£¼ìš” ì •ë‹µ ë¶€ë¶„ ì¶”ì¶œ\"\"\"\n",
    "    import re\n",
    "    \n",
    "    # \"{ì •ë‹µ}ì´/ê°€ ì˜³ë‹¤\" íŒ¨í„´ ì°¾ê¸°\n",
    "    patterns = [\n",
    "        r'\"([^\"]+)\"[ì´ê°€]\\s*ì˜³ë‹¤',\n",
    "        r'([^.]+)[ì´ê°€]\\s*ì˜³ë‹¤'\n",
    "    ]\n",
    "    \n",
    "    for pattern in patterns:\n",
    "        match = re.search(pattern, answer_text)\n",
    "        if match:\n",
    "            return match.group(1).strip()\n",
    "    \n",
    "    # íŒ¨í„´ì´ ì—†ìœ¼ë©´ ì²« ë²ˆì§¸ ë¬¸ì¥ ë°˜í™˜\n",
    "    sentences = answer_text.split('.')\n",
    "    return sentences[0].strip() if sentences else answer_text.strip()\n",
    "\n",
    "def compare_models(rag_predictions_path: str, simple_predictions_path: str, ground_truth_path: str):\n",
    "    \"\"\"\n",
    "    RAG ëª¨ë¸ê³¼ ë‹¨ìˆœ ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"ëª¨ë¸ ì„±ëŠ¥ ë¹„êµ\")\n",
    "    print(\"=\"*60)\n",
    "    \n",
    "    # RAG ëª¨ë¸ í‰ê°€\n",
    "    print(\"\\n1. RAG + íŒŒì¸íŠœë‹ ëª¨ë¸ í‰ê°€:\")\n",
    "    rag_results = evaluate_with_official_metric(rag_predictions_path, ground_truth_path)\n",
    "    print_evaluation_results(rag_results, \"RAG + íŒŒì¸íŠœë‹\")\n",
    "    \n",
    "    # ë‹¨ìˆœ ëª¨ë¸ í‰ê°€ (ìˆëŠ” ê²½ìš°)\n",
    "    if os.path.exists(simple_predictions_path):\n",
    "        print(\"\\n2. íŒŒì¸íŠœë‹ë§Œ ëª¨ë¸ í‰ê°€:\")\n",
    "        simple_results = evaluate_with_official_metric(simple_predictions_path, ground_truth_path)\n",
    "        print_evaluation_results(simple_results, \"íŒŒì¸íŠœë‹ë§Œ\")\n",
    "        \n",
    "        # ì„±ëŠ¥ ì°¨ì´ ì¶œë ¥\n",
    "        if 'exact_match' in rag_results and 'exact_match' in simple_results:\n",
    "            improvement = rag_results['exact_match'] - simple_results['exact_match']\n",
    "            print(f\"\\nğŸ“ˆ RAG íš¨ê³¼: {improvement:.4f} ({improvement*100:.2f}%p ê°œì„ )\")\n",
    "\n",
    "def print_evaluation_results(results, model_name):\n",
    "    \"\"\"í‰ê°€ ê²°ê³¼ ì¶œë ¥\"\"\"\n",
    "    print(f\"\\n--- {model_name} ê²°ê³¼ ---\")\n",
    "    \n",
    "    if 'error' in results:\n",
    "        print(f\"âŒ ì˜¤ë¥˜: {results['error']}\")\n",
    "        return\n",
    "    \n",
    "    if 'exact_match' in results:\n",
    "        print(f\"Exact Match: {results['exact_match']:.4f} ({results['exact_match']*100:.2f}%)\")\n",
    "    \n",
    "    if 'rouge_1' in results:\n",
    "        print(f\"ROUGE-1: {results['rouge_1']:.4f}\")\n",
    "    \n",
    "    if 'bertscore' in results:\n",
    "        print(f\"BERTScore: {results['bertscore']:.4f}\")\n",
    "    \n",
    "    if 'bleurt' in results:\n",
    "        print(f\"BLEURT: {results['bleurt']:.4f}\")\n",
    "    \n",
    "    if 'final_score' in results:\n",
    "        print(f\"ìµœì¢… ì ìˆ˜: {results['final_score']:.4f} ({results['final_score']*100:.2f}%)\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜\"\"\"\n",
    "    \n",
    "    # RAG ì‹œìŠ¤í…œ ë¡œë“œ\n",
    "    print(\"RAG ì‹œìŠ¤í…œ ë¡œë“œ ì¤‘...\")\n",
    "    rag = SimpleKoreanRAG()\n",
    "    rag.load(\"./rag_system\")\n",
    "    print(\"RAG ì‹œìŠ¤í…œ ë¡œë“œ ì™„ë£Œ!\")\n",
    "    \n",
    "    # RAG íŒŒì´í”„ë¼ì¸ ìƒì„±\n",
    "    pipeline = KoreanGrammarRAGPipeline(\n",
    "        rag_system=rag, \n",
    "        model_name=\"./korean_grammar_model\"\n",
    "    )\n",
    "    \n",
    "    # Validation ë°ì´í„° ê²½ë¡œë“¤\n",
    "    val_data_path = \"/home/jiin/korean_grammar_rag/data/korean_language_rag_V1.0_dev.json\"  # ì‹¤ì œ ê²½ë¡œë¡œ ìˆ˜ì •\n",
    "    \n",
    "    # 1. RAG íŒŒì´í”„ë¼ì¸ìœ¼ë¡œ ì¶”ë¡ \n",
    "    print(\"\\n1. RAG + íŒŒì¸íŠœë‹ ëª¨ë¸ ì¶”ë¡ \")\n",
    "    rag_predictions = run_inference_on_validation(\n",
    "        pipeline, \n",
    "        val_data_path, \n",
    "        \"rag_validation_predictions.json\"\n",
    "    )\n",
    "    \n",
    "    # 2. í‰ê°€ ì‹¤í–‰\n",
    "    print(\"\\n2. ì„±ëŠ¥ í‰ê°€\")\n",
    "    results = evaluate_with_official_metric(\n",
    "        \"rag_validation_predictions.json\",\n",
    "        val_data_path\n",
    "    )\n",
    "    \n",
    "    print_evaluation_results(results, \"RAG + íŒŒì¸íŠœë‹\")\n",
    "    \n",
    "    # 3. ëª‡ ê°œ ìƒ˜í”Œ ê²°ê³¼ í™•ì¸\n",
    "    print(\"\\n3. ìƒ˜í”Œ ê²°ê³¼ í™•ì¸\")\n",
    "    show_sample_results(\"rag_validation_predictions.json\", val_data_path, num_samples=5)\n",
    "\n",
    "def show_sample_results(predictions_path: str, ground_truth_path: str, num_samples: int = 5):\n",
    "    \"\"\"ëª‡ ê°œ ìƒ˜í”Œì˜ ì˜ˆì¸¡ ê²°ê³¼ í™•ì¸\"\"\"\n",
    "    \n",
    "    with open(predictions_path, 'r', encoding='utf-8') as f:\n",
    "        predictions = json.load(f)\n",
    "    \n",
    "    with open(ground_truth_path, 'r', encoding='utf-8') as f:\n",
    "        ground_truth = json.load(f)\n",
    "    \n",
    "    # IDë¡œ ë§¤ì¹­\n",
    "    true_dict = {item['id']: item for item in ground_truth}\n",
    "    \n",
    "    print(f\"\\n{'='*50} ìƒ˜í”Œ ê²°ê³¼ {'='*50}\")\n",
    "    \n",
    "    for i, pred_item in enumerate(predictions[:num_samples]):\n",
    "        item_id = pred_item['id']\n",
    "        true_item = true_dict.get(item_id)\n",
    "        \n",
    "        if true_item:\n",
    "            print(f\"\\n--- ìƒ˜í”Œ {i+1} (ID: {item_id}) ---\")\n",
    "            print(f\"ì§ˆë¬¸: {true_item['input']['question'][:100]}...\")\n",
    "            print(f\"ì •ë‹µ: {true_item['output']['answer'][:150]}...\")\n",
    "            print(f\"ì˜ˆì¸¡: {pred_item['output']['answer'][:150]}...\")\n",
    "            \n",
    "            # ì •ë‹µ ì—¬ë¶€ í™•ì¸\n",
    "            true_main = extract_main_answer(true_item['output']['answer'])\n",
    "            pred_main = extract_main_answer(pred_item['output']['answer'])\n",
    "            match_status = \"âœ… ì •ë‹µ\" if true_main == pred_main else \"âŒ ì˜¤ë‹µ\"\n",
    "            print(f\"ê²°ê³¼: {match_status}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cac79389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "k_rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
